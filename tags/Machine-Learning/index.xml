<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on vanhuyz's notes</title><link>http://example.org/tags/Machine-Learning/</link><description>Recent content in Machine Learning on vanhuyz's notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 22 Jul 2019 12:14:03 +0000</lastBuildDate><atom:link href="http://example.org/tags/Machine-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Kinh nghiệm thi AWS Certified Machine Learning – Specialty</title><link>http://example.org/post/kinh-nghiem-thi-aws-machine-learning/</link><pubDate>Mon, 22 Jul 2019 12:14:03 +0000</pubDate><guid>http://example.org/post/kinh-nghiem-thi-aws-machine-learning/</guid><description>Tiếp theo kinh nghiệm thi chứng chỉ Solutions Architect - Associate lần trước thì lần này là chứng chỉ Machine Learning – Specialty. Để đạt được chứng chỉ này bạn cần có những kiến thức cơ bản về Machine Learning (ML), khả năng ứng dụng ML vào các bài toán thực tế cũng như sử dụng thành thạo các dịch vụ liên quan tới ML của AWS.
Các bạn có thể tham khảo câu hỏi mẫu tại đây.</description></item><item><title>自然言語処理の国際学会 ACL2018 @メルボルンに参加してきました！</title><link>http://example.org/post/acl-2018-report/</link><pubDate>Wed, 06 Mar 2019 11:31:59 +0000</pubDate><guid>http://example.org/post/acl-2018-report/</guid><description>7月10日~15日 で オーストラリアのメルボルンで開催されました 56th Annual Meeting of the Association for Computational Linguistics (ACL2018) に参加してきました。
経緯 ACL は一言で言うと「自然言語処理の世界トップコンファレンス」です。
私は最先端の対話システムはどうなっているのか気になって参加にいきました。今回はトップコンファレンス初参加です。
ACL の日程と構成 日程は以下のと通りです。
7/15 (日) チュートリアル 7/16 (月) ~ 7/18 (水) メインカンファレンス 7/19 (木) ~ 7/20 (金) ワークショップ
メインカンファレンスは Long Papers, Short Papers, Posters に分かれています。たくさんのセッションがありますが、私は以下のセッションが気になります。
Question Answering (質問応答） Dialog System (対話システム） Generation (文章生成） Machine Translation (機械翻訳) Summarization (機械要約） Vision (ビジョン) Machine Learning (機械学習) 自然言語処理学会なのに Vision というセッションがあって面白いですね。最近、言語 x 画像 というハイブリッド的な研究が結構増えているようです。
印象に残ったセッションと感想など 初日（7月10日）はチュートリアルの日です。私が対話と強化学習関連の２つチュートリアルを聴講しました。</description></item><item><title>Lối đi nào dành cho các chuyên gia và kỹ sư AI?</title><link>http://example.org/post/loi-di-nao-danh-cho-cac-chuyen-gia-va-ky-su-ai/</link><pubDate>Thu, 13 Sep 2018 11:37:34 +0000</pubDate><guid>http://example.org/post/loi-di-nao-danh-cho-cac-chuyen-gia-va-ky-su-ai/</guid><description>Dưới đây là tổng hợp những suy nghĩ cá nhân của tôi về các thách thức và các lối đi cho chuyên gia cũng như kỹ sư phần mềm liên quan tới AI (trí tuệ nhân tạo).
Thách thức Hiện nay khi mà phong trào gọi là &amp;ldquo;Cách mạng công nghiệp 4.0&amp;rdquo; đang bùng nổ mạnh mẽ hơn bao giờ hết thì công việc liên quan đến AI cũng trở thành 1 trong những ngành nghề hấp dẫn nhất.</description></item><item><title>最尤推定は何ができるの？</title><link>http://example.org/post/maximum-likelihood-examples/</link><pubDate>Tue, 20 Dec 2016 06:51:00 +0000</pubDate><guid>http://example.org/post/maximum-likelihood-examples/</guid><description>英語: Maximum Likelihood Estimation (MLE)
最近仕事で確率モデルを扱う機会があって、パラメータ推定には最尤推定を使うことがありました。 でもわかるような、わからないような状態なので、式を立てて一度整理したいと思います。
例１ 問題 データ $D= { x^{(1)},&amp;hellip;, x^{(N)} }$（母集団） が与えられるとします。 このデータが正規分布に従うと仮定したら、最尤推定でパラメータを推定しましょう。
回答 まず正規分布の式です。
$$ p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp \big(-\frac{(x-\mu)^2}{2\sigma^2}\big) $$
最尤推定はとは、尤度がもっとも高くなるようにパラメータを決定する方法です。「できるかぎりデータにフィットさせる」推定方法です1。
最尤推定の1つ目の条件は、データは独立に同一の確率分布(i.i.d)2に従うのです。 そこで、尤度(likelihood)は
$$ p(D) = \prod_ {x^{(i)}\in D} p(x^{(i)}) $$
です。これを最大化したいです。
確率$p$は$[0,1]$の間の値なので、積をとるとコンピュータの計算に誤差が出るので実際、$\log$3をとることが多いです。
$$ \begin{align} \log p(D) &amp;amp;= \sum_ {x^{(i)}\in D} \log p(x^{(i)}) \
&amp;amp;= \sum_ {x^{(i)}\in D} \log \Big(\frac{1}{\sqrt{2\pi\sigma^2}}\exp \big(-\frac{(x^{(i)}-\mu)^2}{2\sigma^2}\big)\Big) \
&amp;amp;= -\frac{N}{2}\log(2\pi) - N\log (\sigma) - \sum_ {x^{(i)}\in D} \frac{(x^{(i)}-\mu)^2}{2\sigma^2} \end{align} $$
この場合のパラメータは$(\mu, \sigma)$ですね。</description></item><item><title>Coursera 機械学習 - プログラミング課題8解答例</title><link>http://example.org/post/coursera-machinelearning-assignment-8/</link><pubDate>Mon, 18 Apr 2016 04:57:00 +0000</pubDate><guid>http://example.org/post/coursera-machinelearning-assignment-8/</guid><description>ソースコード： https://github.com/vanhuyz/coursera-ml/tree/master/machine-learning-ex8/ex8
概要 今回の課題は２パートに分かれています。前半は異常検出アルゴリズム（anomaly detection）を用いて障害が発生したサーバーを検出、後半は映画おすすめシステムを作ります。
1. 異常検出 サーバーは２つのフィーチャーがあります：レスポンスのスループット(mb/s)とレイテンシ(ms)です。m = 307 examplesのデータがあって、その中少し異常点があります。
1.1 正規分布 データ$x$は正規分布に従うのを仮定します。
正規分布：
$$ p(x;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$
ここで、$\mu$は平均値、$\sigma^2$は分散です。
1.2 正規分布のパラメータ推定 $i$番目のフィーチャーに対して：
平均値： $$ \mu_ i = \frac{1}{m} \sum_ {j=1}^{m} x_ i^{(j)} $$
分散： $$ \sigma_ i^2 = \frac{1}{m}\sum_ {j=1}^{m}(x_ i^{(j)}-\mu_ i)^2 $$
Octaveで書き直すと、
mu = mean(X); sigma2 = var(X) * (m - 1) / m; 注意するのはOctaveの分散計算関数(var)はデフォルトでmではなくm-1を割るので工夫が必要です。
正規分布のグラフ（2Dで輪郭表現）は以下になります。 1.3 しきい値の選択 しきい値$\epsilon$の選択にはクロスバリデーションセットで$F_ 1$スコアの計算が必要です。
$F_ 1$はprecision ($prec$) とrecall ($rec$) から計算します。</description></item><item><title>Coursera 機械学習 - プログラミング課題6解答例</title><link>http://example.org/post/coursera-machinelearning-assignment-6/</link><pubDate>Mon, 14 Mar 2016 03:05:00 +0000</pubDate><guid>http://example.org/post/coursera-machinelearning-assignment-6/</guid><description>ソースコード：https://github.com/vanhuyz/coursera-ml/tree/master/machine-learning-ex6/ex6
0. 概要 今回の課題はSupport Vector Machines(SVM)を使って迷惑メールを識別することです　(spam classifier)。パート１はSVMをいろいろ試すのと、パート２は迷惑メールの課題になります。
1. Support Vector Machines (SVM) SVM with Gaussian Kernels Gaussian Kernel:
$$ K_ {gaussian}(x^{(i)},x^{(j)}) = \exp\Big(-\frac{|x^{(i)}-x^{(j)}|^2}{2\sigma^2}\Big) = \exp\Bigg(-\frac{\sum_ {k=1}^n(x_ k^{(i)}-x_ k^{(j)})^2}{2\sigma^2}\Bigg) $$
Octaveで書き直すと、
function sim = gaussianKernel(x1, x2, sigma) x1 = x1(:); x2 = x2(:); sim = exp(-norm(x1 - x2)^2 / (2*sigma^2)); end 途中で実行するエラーがありました
error: get: unknown hggroup property Color visualizeBoundary.m ファイルの21行
contour(X1, X2, vals, [1,1], 'b') に変更するとなおりました。
参考：https://www.coursera.org/learn/machine-learning/discussions/1RmNir4KEeWSBRJpSArseQ
データセット２の実行結果： Cross Validation Setでパラメータ選択 SVMのライブラリを使う前提なのでSVMの中身は気にしなくていいです。SVM with a Gaussian kernelを使う場合、調整必要なパラメータは$C$と$\sigma$だけです。$C$は正規化を調整するパラメータで、$\sigma$は２つ点の類似度（similarity)を調整するパラメータです。 一般的に,</description></item><item><title>Coursera 機械学習 - プログラミング課題4解答例</title><link>http://example.org/post/coursera-machinelearning-assignment-4/</link><pubDate>Sun, 21 Feb 2016 08:29:00 +0000</pubDate><guid>http://example.org/post/coursera-machinelearning-assignment-4/</guid><description>解答例：https://github.com/vanhuyz/coursera-ml/tree/master/machine-learning-ex4/ex4
概要 今回の課題は前回と続き手書き数字の判定問題です。前回はニューラルネットワークで、与えられたパラメータからフィードフォワード・プロパゲイションを行うところまででした。今回はbackpropagationアルゴリズムを使って、パラメータを抽出することです（学習過程）。
1. ニューラルネットワーク 1.1 データを可視化 5000学習データがあり、各データは20x20ピックセルグレースケールの数字画像です。
1.2 モデル表現 ニューラルネットワークは３つのレイヤーがあります：input layer, hidden layerとoutput layerです。
1.3 フィードフォワードとコスト関数 ニューラルネットワークのコスト関数（未正規化）はこのように計算できます。
$$ J(\Theta) = \frac{1}{m}\sum_ {i=1}^{m}\sum_ {k=1}^{K}\Big[-y_ k^{(i)}\log((h_ \Theta(x^{(i)}))_ k)-(1-y_ k^{(i)})\log(1-(h_ \Theta(x^{(i)}))_ k)\Big] $$
ここで$K=10$（10数字）、$m=5000$（学習データ）、$h_ \theta$はFigure 2から計算できます。$y$は0から9の数字ですが、計算都合のため以下の図のようにベクトルにマッピングします。
課題はコスト関数をOctaveで計算することです。
function[J grad] = nnCostFunction(nn_ params, ... input_ layer_ size, ... hidden_ layer_ size, ... num_ labels, ... X, y, lambda) % Reshape nn_ params back into the parameters Theta1 and Theta2 Theta1 = reshape(nn_ params(1:hidden_ layer_ size * (input_ layer_ size + 1)), .</description></item><item><title>Coursera 機械学習 - プログラミング課題２解答例</title><link>http://example.org/post/coursera-machinelearning-assignment-2/</link><pubDate>Sun, 31 Jan 2016 15:38:00 +0000</pubDate><guid>http://example.org/post/coursera-machinelearning-assignment-2/</guid><description>課題について 今回の課題はLogistic Regression（分類）に関するものです。
https://www.coursera.org/learn/machine-learning/programming/ixFof/logistic-regression
ソースコードはgithubに上げました。
https://github.com/vanhuyz/coursera-ml/tree/master/machine-learning-ex2/ex2
問題１ Logistic regressionモデルで学生の過去の２つテスト結果から大学入試に合格・不合格を予測します。
学習データは以下の図の通りです。＋は合格、◯は不合格です。
1. Sigmoid関数 Sigmoid関数は以下のように定義されています。
$$ g(z)=\frac{1}{1+e^{-z}} $$
注意したいのはzはスカラー、ベクトル、行列でもいいです。ベクトル・行列の場合は各要素を適応することになるます。
実装：
function g = sigmoid(z) g = zeros(size(z)); g = 1 ./ (1 + exp(-z)); end Octaveの**+**、**exp**演算子はもし行列と実数を計算するとき、自動に各要素に適応するようです。　**/**　はそうになっていないのでちゃんと　**.**　をつけましょう。
2. Cost function and gradient Cost function: $$ J(\theta) = \frac{1}{m}\sum_ {i=1}^{m}[-y^{(i)}\log(h_ \theta(x^{(i)}))-(1-y^{(i)})\log(1-h_ \theta(x^{(i)}))] $$
Gradient: $$ \frac{\partial J(\theta)}{\partial \theta_ j} = \frac{1}{m}\sum_ {i=1}^m (h_ \theta(x^{(i)})-y^{(i)})x_ j^{(i)} $$</description></item><item><title>Coursera 機械学習 - プログラミング課題1解答例</title><link>http://example.org/post/coursera-machinelearning-assignment-1/</link><pubDate>Mon, 18 Jan 2016 13:07:00 +0000</pubDate><guid>http://example.org/post/coursera-machinelearning-assignment-1/</guid><description>課題のページ https://www.coursera.org/learn/machine-learning/programming/8f3qT/linear-regression
プログラミング課題はちょっと重いので今回の解答例を上げます。 わからないことや別の解答がありましたらコメントをお願いします。
必須課題 1. Computing Cost (for One Variable) Gradient DescentのCost function $J(\theta)$ は以下の通り
$$ J(\theta) = \frac{1}{2m}\sum_ {i=1}^m (h_ \theta(x^{(i)}) - y^{(i)} )^2 $$
ここで、仮定関数 $ h_ \theta(x) $は
$$ h_ \theta(x) = \theta^T x = \theta_ 0 + \theta_ 1x_ 1 $$
それで、
$$ J(\theta) = \frac{1}{2m}\sum_ {i=1}^m (\theta^T x^{(i)} - y^{(i)} )^2 $$
注意したいのは $\theta$ と $x^{(i)}$ はベクトルで、$ y^{(i)} $は実数です。 課題はこの関数をOctaveで書くことです。以下は解答例です。
function J = computeCost(X, y, theta) m = length(y); % number of training examples % 97 examplesがあるのでここで m == 97 % size(X) == [97 2] % size(y) == [97 1] % size(theta) == [2 1] % Xは97x2行列。１行は１つのtraining example [x0 x1]　(x0はいつも1) % yは97次元ベクトル % thetaは２次元ベクトル J = 0; % まずは和の部分を計算します for i = 1:m J += (theta&amp;#39; * X(i,:)&amp;#39; - y(i))^2; end % X(i,:)は１つの行、つまりtraining exampleです。ベクトルに変換するので転置を取りました。 % 最後に2mを割るだけです J = J / (2*m); end 実行した結果、cost functionの値は32.</description></item></channel></rss>