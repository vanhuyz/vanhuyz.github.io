<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PRML on vanhuyz's notes</title><link>http://example.org/tags/PRML/</link><description>Recent content in PRML on vanhuyz's notes</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 15 Oct 2017 06:41:00 +0000</lastBuildDate><atom:link href="http://example.org/tags/PRML/index.xml" rel="self" type="application/rss+xml"/><item><title>PRML - Chap 12: Continuous Latent Variables - 12.2.3 ~ end</title><link>http://example.org/post/prml-chap-12-continuous-latent-variables-12-2-3-end/</link><pubDate>Sun, 15 Oct 2017 06:41:00 +0000</pubDate><guid>http://example.org/post/prml-chap-12-continuous-latent-variables-12-2-3-end/</guid><description>12.2.3 Bayesian PCA Graphical model cho Bayesian PCA được biểu diễn như sau: Ở đây ta đã đưa thêm Gaussian prior độc lập cho $\mathbf{W}$ với các precision $\alpha_i$:
Giá trị của $\alpha_i$ được tìm bằng cách maximizing the maginal likelihood:
Chú ý là để đơn giản ở đây ta coi $\mathbf{\mu}$ và $\sigma^2$ là các parameters chứ không đưa thêm priors cho chúng.
Sử dụng Laplace approximation, các $\alpha_i$ sẽ tìm được như sau:</description></item><item><title>PRML - Chap 10: Approximate Inference - 10.1</title><link>http://example.org/post/prml-chap-10-approximate-inference-10-1-10-3/</link><pubDate>Sun, 17 Sep 2017 04:53:10 +0000</pubDate><guid>http://example.org/post/prml-chap-10-approximate-inference-10-1-10-3/</guid><description>Trong thực tế, tính toán trong không gian nhiều chiều của các hàm phức tạp (chẳng hạn trong EM là tính posterior và kỳ vọng của nó) là rất khó khăn nên người ta dùng phương pháp xấp xỉ.
10.1 Variational Inference Kí hiệu set của N i.i.d data là $\mathbf{X}=\{x_ 1, \ldots, x_ n\}$, tất cả latent variables là $\mathbf{Z}=\{z_ 1, \ldots, z_ n\}$. Mô hình của chúng ta sẽ là joint distribution $p(\mathbf{X},\mathbf{Z})$, và mục tiêu là đi tìm giá trị xấp xỉ cho posterior distribution $p(\mathbf{Z}|\mathbf{X})$ và model evidence $p(\mathbf{X})$.</description></item><item><title>PRML - Chap 9: Mixture Models and EM - 9.3</title><link>http://example.org/post/prml-chap-9-mixture-models-and-em-9-3/</link><pubDate>Sat, 02 Sep 2017 16:28:06 +0000</pubDate><guid>http://example.org/post/prml-chap-9-mixture-models-and-em-9-3/</guid><description>9.3 An Alternative View of EM Mục đích của thuật toán EM là tìm maximum likelihood cho model có biến ẩn (latent variables). $\mathbf{X}$: dữ liệu quan sát được, $Z$: tất cả biến ẩn, $\mathbf{\theta}$: model parameters thì hàm log likelihood là:
$$ \ln p(\mathbf{X}|\mathbf{\theta}) = \ln \Big\{ \sum_ {\mathbf{Z}} p(\mathbf{X},\mathbf{Z}|\mathbf{\theta}) \Big\} $$
Vấn đề ở đây là vế phải là log của tổng nên cho dù giả sử $p(\mathbf{X},\mathbf{Z}|\mathbf{\theta})$ là hàm mũ thì cũng không thể tìm được nghiệm maximum likelihood (có thể tính thử đạo hàm để kiểm nghiệm :D).</description></item><item><title>PRML - Chap 8: Graphical Models - 8.3</title><link>http://example.org/post/prml-chap-8-graphical-models-8-3/</link><pubDate>Sat, 08 Jul 2017 20:35:27 +0000</pubDate><guid>http://example.org/post/prml-chap-8-graphical-models-8-3/</guid><description>$ \def\ci{\perp\!\!\!\perp} \def\given{\ | \ } \def\nci{\perp\!\!\!\perp\!\!\!\!\!\!/ \ } \def\zeroslash{0\!\!\!/} $
8.3 Markov Random Fields Markov random field, còn gọi là Markov network hay undirected graphical model được biểu diễn bằng graph vô hướng.
8.3.1 Conditional independence properties Sử dụng graph vô hướng sẽ dễ kiểm tra tính chất độc lập có điều kiện của 2 biến hơn (conditional independence property). Chú ý là chỉ giống với phép thử d-separation trong trường hợp không có hiện tượng &amp;rsquo;explaining away'.</description></item><item><title>PRML - Chap 7: Sparse Kernel Machines 7.1.1 ~ 7.1.3</title><link>http://example.org/post/prml-chap-7-sparse-kernel-machines-7-1-1-7-1-2/</link><pubDate>Mon, 19 Jun 2017 08:57:53 +0000</pubDate><guid>http://example.org/post/prml-chap-7-sparse-kernel-machines-7-1-1-7-1-2/</guid><description>7.1.1 Overlapping class distributions Trong phần trước chúng ta đã giả sử là dữ liệu rất đẹp và tồn tại đường biên giới có thể chia được các class ra tách biệt với nhau. Tuy nhiên trong thực tế thì vì dữ liệu có noise nên nếu cố tìm đường biên giới để tách các class thì dễ dẫn đến overfit. Trong phần này, chúng ta sẽ xây dựng model mà cho phép 1 vài điểm có thể bị phân loại sai, vì mục đích chính là model chạy tốt trên tập test.</description></item><item><title>PRML - Chap 6: Kernel methods - 6.4 Gaussian Processes</title><link>http://example.org/post/prml-chap-6-kernel-methods/</link><pubDate>Sun, 11 Jun 2017 13:12:16 +0000</pubDate><guid>http://example.org/post/prml-chap-6-kernel-methods/</guid><description>6.4.1 Linear regression revisited Thử xem xét lại model:
$$ y(\mathbf{x}) = \mathbf{w}^T\phi(\mathbf{x}) $$
với $\mathbf{x}$ là input, $y(\mathbf{x})$ là output, $\mathbf{w}$ là parameters, $\phi(\mathbf{x})$ là basis function.
Giả sử prior đối với $\mathbf{w}$ là 1 Gaussian đơn vị:
$$ p(\mathbf{w}) = \mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I}) $$
Trong thực tế, với training set $x_ 1,\ldots,x_ N$, ta quan tâm tới
$$ \mathbf{y} = \big(y(x_ 1),\ldots,y(x_ N)\big)^T = \mathbf{\Phi}\mathbf{w} $$
với $\mathbf{\Phi}$ là ma trận với mỗi phần tử là $\Phi_ {nk} = \phi_ k(x_ n)$ ($\phi_ k$ là gì thì chưa rõ, có thể sách in sai?</description></item><item><title>PRML - Chap 5: Neural networks</title><link>http://example.org/post/prml-chap-5-neural-networks/</link><pubDate>Sat, 29 Apr 2017 09:23:29 +0000</pubDate><guid>http://example.org/post/prml-chap-5-neural-networks/</guid><description>5.5.3 Invariances Trong bài toán thực tế, nhiều lúc kết quả dự đoán không đổi cho dù đầu vào có nhứng biến đổi nhất định. Chẳng hạn trong bài toán phân biệt ảnh chó với ảnh mèo thì
còn mèo có nằm ở góc nào của ảnh thì vẫn là con mèo (translation invariance) kích cỡ ảnh hay con mèo có to nhỏ cỡ nào thì vẫn là con mèo (scale invariance) Dữ liệu mà có cực nhiều với đầy đủ các patterns thì máy có thể tự học được các thay đổi đó nhưng thực tế thì không như vậy.</description></item><item><title>PRML - Chap 4: Linear Models for Classification</title><link>http://example.org/post/prml-chap-4-linear-models-for-classification/</link><pubDate>Sat, 29 Apr 2017 09:13:03 +0000</pubDate><guid>http://example.org/post/prml-chap-4-linear-models-for-classification/</guid><description>4.3.1 Fixed basis functions Bài toán là phân loại các input vector x Áp 1 hàm phi tuyến cố định $\phi(x)$ vào thì bài toán trở thành phân loại các $\phi(x)$ → đường decision boundary sẽ trờ thành tuyến tính → bài toán trở nên đơn giản đi rất nhiều 4.3.2 Logistic regression Thuật toán này rất quan trọng nên anh em cần chú ý hiểu kỹ Tên là regression nhưng lại là thuật toán classification Trở lại bài toán phân biệt 2 lớp: data set $\{\phi_ n, t_ n\}$ với $t_ n \in \{0,1\}$ và $n=1,\ldots,N$ Hàm likelihood trở thành $$ p(\mathbf{t}|w) = \prod_ {n=1}^N y_ n^{t_ n}{1-y_ n}^{1-t_ n} $$ với $\mathbf{t} = (t_ 1,\ldots,t_ N)^T$ và $y_ n = p(C_ 1 | \phi_ n)=y(\phi) = \sigma(w^T\phi)$ là xác suất để $\phi_ n$ rơi vào lớp $C_ 1$.</description></item><item><title>PRML - Chap 3: Linear Models for Regression</title><link>http://example.org/post/prml-chap3-linear-models-for-regression/</link><pubDate>Sat, 29 Apr 2017 08:58:59 +0000</pubDate><guid>http://example.org/post/prml-chap3-linear-models-for-regression/</guid><description>The Evidence Approximation Trong Bayesian đối với linear model, ta đã thấy sự xuất hiện của $\alpha$ và $\beta$ là các hyperparameters của prior và noise.
Trong chương này, ta sẽ cố gắng tìm các giá trị này dựa trên maximizing the magrinal likelihood function.
Framework này được gọi là evidence approximation.
Theo công thức Bayes:
$$ p(\alpha, \beta|\mathbf{t}) \propto p(\mathbf{t}|\alpha,\beta)p(\alpha,\beta) $$
$p(\mathbf{t}|\alpha,\beta)$ là marginal likelihood function
Evaluation of the evidence function Marginal likelihood function có thể triển khai theo $\mathbf{w}$ như sau:</description></item><item><title>PRML - Chap 2: Probability Distributions</title><link>http://example.org/post/prml-chap2-probability-distributions/</link><pubDate>Sat, 29 Apr 2017 08:41:59 +0000</pubDate><guid>http://example.org/post/prml-chap2-probability-distributions/</guid><description>2.3.3 Bayes&amp;rsquo; theorem for Gaussian variables Tóm lại Nếu $p(x)$ và $p(y|x)$ đều là các phân phối chuẩn thì $p(y)$ và $p(x|y)$ cũng là các phân phối chuẩn.
Công thức: Giả sử
$$ \begin{align} p(x) &amp;amp; = \mathcal{N}(x|\mathbf{\mu}, \mathbf{\Lambda} ^{-1}\big) \\ p(y|x)&amp;amp; = \mathcal{N}\big(y|\mathbf{A}x+\mathbf{b}, \mathbf{L} ^{-1}\big) \\ \end{align} $$
thì
$$ \begin{align} p(y) &amp;amp; = &amp;amp;\mathcal{N}\big(y|\mathbf{A}\mu+b,\mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}}\big)\\ p(x|y) &amp;amp; = &amp;amp;\mathcal{N}\big(x|\Sigma{ \mathbf{A}^{\mathrm{T}}\mathbf{L}(y-b)+\mathbf{\Lambda}\mu},\Sigma\big) \end{align} $$
với
$$ \Sigma = (\mathbf{\Lambda}+\mathbf{A}^{T}\mathbf{L}\mathbf{A})^{-1} $$
2.3.4 Maximum likelihood for the Gaussian Data set</description></item><item><title>PRML - Chap 1: Probability Theory</title><link>http://example.org/post/prml-chap1-probability-theory/</link><pubDate>Sat, 18 Feb 2017 15:56:40 +0000</pubDate><guid>http://example.org/post/prml-chap1-probability-theory/</guid><description>Ví dụ Có 2 hộp: Đỏ, Lam Có 2 loại quả: Táo(màu lá), Cam(màu cam)
Chọn 1 hộp bất kỳ rồi bốc 1 quả bất kỳ trong hộp đó
Biến ngẫu nhiên (Random variable) B: hộp, có thể nhận 1 trong 2 giá trị r(đỏ), b (lam) F: quả, có thể nhận 1 trong 2 giá trị a(táo), o (cam)
Ký hiệu xác suất Giả sử xác xuất chọn hộp đỏ trong 2 hộp là 4/10</description></item></channel></rss>