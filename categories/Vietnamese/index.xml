<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Vietnamese on vanhuyz's notes</title><link>http://example.org/categories/Vietnamese/</link><description>Recent content in Vietnamese on vanhuyz's notes</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 22 Jul 2019 12:14:03 +0000</lastBuildDate><atom:link href="http://example.org/categories/Vietnamese/index.xml" rel="self" type="application/rss+xml"/><item><title>Kinh nghiệm thi AWS Certified Machine Learning – Specialty</title><link>http://example.org/post/kinh-nghiem-thi-aws-machine-learning/</link><pubDate>Mon, 22 Jul 2019 12:14:03 +0000</pubDate><guid>http://example.org/post/kinh-nghiem-thi-aws-machine-learning/</guid><description>Tiếp theo kinh nghiệm thi chứng chỉ Solutions Architect - Associate lần trước thì lần này là chứng chỉ Machine Learning – Specialty. Để đạt được chứng chỉ này bạn cần có những kiến thức cơ bản về Machine Learning (ML), khả năng ứng dụng ML vào các bài toán thực tế cũng như sử dụng thành thạo các dịch vụ liên quan tới ML của AWS.
Các bạn có thể tham khảo câu hỏi mẫu tại đây.</description></item><item><title>Kinh nghiệm thi chứng chỉ AWS Certified Solutions Architect – Associate</title><link>http://example.org/post/kinh-nghiem-thi-chung-chi-aws-certified-solutions-architect-associate/</link><pubDate>Mon, 08 Jul 2019 15:08:13 +0000</pubDate><guid>http://example.org/post/kinh-nghiem-thi-chung-chi-aws-certified-solutions-architect-associate/</guid><description>Giới thiệu chung AWS Certification là bộ chứng chỉ được câp bởi Amazon đánh giá mức độ hiểu biết về cloud (điện toán đám mây), cụ thể là các dịch vụ của Amazon Web Services (AWS)
cũng như việc áp dụng các dịch vụ đó 1 cách hiệu quả vào trong các bài toán thực tế. Bộ chứng chỉ này được chia ra theo các vai trò là Cloud Practitioner, Architect, Developer, và Operations, ngoài ra cộng thêm Specialty.</description></item><item><title>Tổng hợp hội thảo thường niên của các công ty công nghệ tại Tokyo</title><link>http://example.org/post/annual-tech-conferences/</link><pubDate>Tue, 20 Nov 2018 12:05:41 +0000</pubDate><guid>http://example.org/post/annual-tech-conferences/</guid><description>Thời điểm cuối năm cũ và đầu năm mới thường là lúc mà các công ty tổ chức hội thảo công nghệ (tech conference) vừa để giới thiệu dịch vụ hay &amp;ldquo;khoe&amp;rdquo; những thành quả trong 1 năm vừa qua vừa để thu hút nhân tài đến với công ty. Trước đây các công ty thường sắp xếp tổ chức cuối tuần để nhiều người tiện tham gia hơn nhưng gần đây thì lại có xu hướng chuyển qua tổ chức ngày thường.</description></item><item><title>Lối đi nào dành cho các chuyên gia và kỹ sư AI?</title><link>http://example.org/post/loi-di-nao-danh-cho-cac-chuyen-gia-va-ky-su-ai/</link><pubDate>Thu, 13 Sep 2018 11:37:34 +0000</pubDate><guid>http://example.org/post/loi-di-nao-danh-cho-cac-chuyen-gia-va-ky-su-ai/</guid><description>Dưới đây là tổng hợp những suy nghĩ cá nhân của tôi về các thách thức và các lối đi cho chuyên gia cũng như kỹ sư phần mềm liên quan tới AI (trí tuệ nhân tạo).
Thách thức Hiện nay khi mà phong trào gọi là &amp;ldquo;Cách mạng công nghiệp 4.0&amp;rdquo; đang bùng nổ mạnh mẽ hơn bao giờ hết thì công việc liên quan đến AI cũng trở thành 1 trong những ngành nghề hấp dẫn nhất.</description></item><item><title>Kinh nghiệm đổi bằng lái xe ô tô cấp tại Nhật sang bằng Việt Nam</title><link>http://example.org/post/kinh-nghiem-doi-bang-lai-xe-o-to-nhat-sang-bang-viet-nam/</link><pubDate>Tue, 07 Aug 2018 12:17:13 +0000</pubDate><guid>http://example.org/post/kinh-nghiem-doi-bang-lai-xe-o-to-nhat-sang-bang-viet-nam/</guid><description>Gần đây số lượng người Việt ở Nhật cũng nhiều và trong đó cũng có nhiều người học lái xe ô tô ở Nhật luôn. Tất nhiên mục đích chính vẫn là lái xe ở Nhật nhưng chắc nhiều bạn cũng băn khoăn không biết bằng Nhật có lái xe được ở Việt Nam hay không. Mình cũng đã tìm hiểu về cái này và được câu trả lời là:</description></item><item><title>Đánh giá một số trung tâm học tiếng Anh giao tiếp ở Nhật</title><link>http://example.org/post/danh-gia-trung-tam-hoc-tieng-anh/</link><pubDate>Mon, 09 Jul 2018 14:26:02 +0000</pubDate><guid>http://example.org/post/danh-gia-trung-tam-hoc-tieng-anh/</guid><description>Mở đầu Người Nhật vốn nổi tiếng không nói được tiếng Anh nhưng gần đây khi mà các công ty Nhật bắt đầu coi trọng vấn đề &amp;ldquo;toàn cầu hoá&amp;rdquo; thì tiếng Anh lại trở thành 1 trong kỹ năng rất được săn đón. Chính vì thế mà các trung tâm tiếng Anh giao tiếp (英会話) cho người đi làm mở ra khá rầm rộ với nhiều hình thức khác nhau.</description></item><item><title>Bàn chút về câu hỏi "Bạn là ai?"</title><link>http://example.org/post/ban-chut-ve-cau-hoi-ban-la-ai/</link><pubDate>Wed, 04 Jul 2018 19:13:06 +0000</pubDate><guid>http://example.org/post/ban-chut-ve-cau-hoi-ban-la-ai/</guid><description>Mới đọc xong quyển Homo Deus: A Brief History of Tomorrow của tác giả Harari thấy có 1 đoạn khá thú vị về cái gọi là &amp;ldquo;bản thân mình&amp;rdquo; (self) nên tổng hợp lại 1 chút về chủ đề này.
Mình vốn là người rất kém trong việc giới thiệu bản thân khi gặp 1 người mới, vì chính mình cũng không hiểu rõ mình là người như thế nào. Gần đây mới làm thử trắc nghiệm tính cách trên 16personalities thì ra kết quả là INTP - the logician, tạm dịch là nhà logic học.</description></item><item><title>PRML - Chap 12: Continuous Latent Variables - 12.2.3 ~ end</title><link>http://example.org/post/prml-chap-12-continuous-latent-variables-12-2-3-end/</link><pubDate>Sun, 15 Oct 2017 06:41:00 +0000</pubDate><guid>http://example.org/post/prml-chap-12-continuous-latent-variables-12-2-3-end/</guid><description>12.2.3 Bayesian PCA Graphical model cho Bayesian PCA được biểu diễn như sau: Ở đây ta đã đưa thêm Gaussian prior độc lập cho $\mathbf{W}$ với các precision $\alpha_i$:
Giá trị của $\alpha_i$ được tìm bằng cách maximizing the maginal likelihood:
Chú ý là để đơn giản ở đây ta coi $\mathbf{\mu}$ và $\sigma^2$ là các parameters chứ không đưa thêm priors cho chúng.
Sử dụng Laplace approximation, các $\alpha_i$ sẽ tìm được như sau:</description></item><item><title>PRML - Chap 10: Approximate Inference - 10.1</title><link>http://example.org/post/prml-chap-10-approximate-inference-10-1-10-3/</link><pubDate>Sun, 17 Sep 2017 04:53:10 +0000</pubDate><guid>http://example.org/post/prml-chap-10-approximate-inference-10-1-10-3/</guid><description>Trong thực tế, tính toán trong không gian nhiều chiều của các hàm phức tạp (chẳng hạn trong EM là tính posterior và kỳ vọng của nó) là rất khó khăn nên người ta dùng phương pháp xấp xỉ.
10.1 Variational Inference Kí hiệu set của N i.i.d data là $\mathbf{X}=\{x_ 1, \ldots, x_ n\}$, tất cả latent variables là $\mathbf{Z}=\{z_ 1, \ldots, z_ n\}$. Mô hình của chúng ta sẽ là joint distribution $p(\mathbf{X},\mathbf{Z})$, và mục tiêu là đi tìm giá trị xấp xỉ cho posterior distribution $p(\mathbf{Z}|\mathbf{X})$ và model evidence $p(\mathbf{X})$.</description></item><item><title>PRML - Chap 9: Mixture Models and EM - 9.3</title><link>http://example.org/post/prml-chap-9-mixture-models-and-em-9-3/</link><pubDate>Sat, 02 Sep 2017 16:28:06 +0000</pubDate><guid>http://example.org/post/prml-chap-9-mixture-models-and-em-9-3/</guid><description>9.3 An Alternative View of EM Mục đích của thuật toán EM là tìm maximum likelihood cho model có biến ẩn (latent variables). $\mathbf{X}$: dữ liệu quan sát được, $Z$: tất cả biến ẩn, $\mathbf{\theta}$: model parameters thì hàm log likelihood là:
$$ \ln p(\mathbf{X}|\mathbf{\theta}) = \ln \Big\{ \sum_ {\mathbf{Z}} p(\mathbf{X},\mathbf{Z}|\mathbf{\theta}) \Big\} $$
Vấn đề ở đây là vế phải là log của tổng nên cho dù giả sử $p(\mathbf{X},\mathbf{Z}|\mathbf{\theta})$ là hàm mũ thì cũng không thể tìm được nghiệm maximum likelihood (có thể tính thử đạo hàm để kiểm nghiệm :D).</description></item><item><title>PRML - Chap 8: Graphical Models - 8.3</title><link>http://example.org/post/prml-chap-8-graphical-models-8-3/</link><pubDate>Sat, 08 Jul 2017 20:35:27 +0000</pubDate><guid>http://example.org/post/prml-chap-8-graphical-models-8-3/</guid><description>$ \def\ci{\perp\!\!\!\perp} \def\given{\ | \ } \def\nci{\perp\!\!\!\perp\!\!\!\!\!\!/ \ } \def\zeroslash{0\!\!\!/} $
8.3 Markov Random Fields Markov random field, còn gọi là Markov network hay undirected graphical model được biểu diễn bằng graph vô hướng.
8.3.1 Conditional independence properties Sử dụng graph vô hướng sẽ dễ kiểm tra tính chất độc lập có điều kiện của 2 biến hơn (conditional independence property). Chú ý là chỉ giống với phép thử d-separation trong trường hợp không có hiện tượng &amp;rsquo;explaining away'.</description></item><item><title>PRML - Chap 7: Sparse Kernel Machines 7.1.1 ~ 7.1.3</title><link>http://example.org/post/prml-chap-7-sparse-kernel-machines-7-1-1-7-1-2/</link><pubDate>Mon, 19 Jun 2017 08:57:53 +0000</pubDate><guid>http://example.org/post/prml-chap-7-sparse-kernel-machines-7-1-1-7-1-2/</guid><description>7.1.1 Overlapping class distributions Trong phần trước chúng ta đã giả sử là dữ liệu rất đẹp và tồn tại đường biên giới có thể chia được các class ra tách biệt với nhau. Tuy nhiên trong thực tế thì vì dữ liệu có noise nên nếu cố tìm đường biên giới để tách các class thì dễ dẫn đến overfit. Trong phần này, chúng ta sẽ xây dựng model mà cho phép 1 vài điểm có thể bị phân loại sai, vì mục đích chính là model chạy tốt trên tập test.</description></item><item><title>PRML - Chap 6: Kernel methods - 6.4 Gaussian Processes</title><link>http://example.org/post/prml-chap-6-kernel-methods/</link><pubDate>Sun, 11 Jun 2017 13:12:16 +0000</pubDate><guid>http://example.org/post/prml-chap-6-kernel-methods/</guid><description>6.4.1 Linear regression revisited Thử xem xét lại model:
$$ y(\mathbf{x}) = \mathbf{w}^T\phi(\mathbf{x}) $$
với $\mathbf{x}$ là input, $y(\mathbf{x})$ là output, $\mathbf{w}$ là parameters, $\phi(\mathbf{x})$ là basis function.
Giả sử prior đối với $\mathbf{w}$ là 1 Gaussian đơn vị:
$$ p(\mathbf{w}) = \mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I}) $$
Trong thực tế, với training set $x_ 1,\ldots,x_ N$, ta quan tâm tới
$$ \mathbf{y} = \big(y(x_ 1),\ldots,y(x_ N)\big)^T = \mathbf{\Phi}\mathbf{w} $$
với $\mathbf{\Phi}$ là ma trận với mỗi phần tử là $\Phi_ {nk} = \phi_ k(x_ n)$ ($\phi_ k$ là gì thì chưa rõ, có thể sách in sai?</description></item><item><title>Thử code CycleGAN biến ảnh cam thành ảnh táo với TensorFlow</title><link>http://example.org/post/thu-code-cyclegan-bien-anh-cam-thanh-anh-tao-voi-tensorflow/</link><pubDate>Tue, 02 May 2017 11:30:02 +0000</pubDate><guid>http://example.org/post/thu-code-cyclegan-bien-anh-cam-thanh-anh-tao-voi-tensorflow/</guid><description>Giới thiệu về CycleGAN Trước hết mời các bạn xem video này:
Đây là thuật toán sử dụng Deep Learning để chuyển từ ảnh này sang ảnh kia mà vẫn giữ nguyên bố cục, chỉ thay đổi bề mặt của vật thể. Chẳng hạn chuyển từ ảnh ngựa thường sang ngựa vằn, chuyển ảnh cam thành ảnh táo, chuyển ảnh thành tranh&amp;hellip; Không chỉ áp dụng với ảnh mà còn cả với video nữa, và như tác giả có nói là có thể realtime với video 60fps!</description></item><item><title>PRML - Chap 5: Neural networks</title><link>http://example.org/post/prml-chap-5-neural-networks/</link><pubDate>Sat, 29 Apr 2017 09:23:29 +0000</pubDate><guid>http://example.org/post/prml-chap-5-neural-networks/</guid><description>5.5.3 Invariances Trong bài toán thực tế, nhiều lúc kết quả dự đoán không đổi cho dù đầu vào có nhứng biến đổi nhất định. Chẳng hạn trong bài toán phân biệt ảnh chó với ảnh mèo thì
còn mèo có nằm ở góc nào của ảnh thì vẫn là con mèo (translation invariance) kích cỡ ảnh hay con mèo có to nhỏ cỡ nào thì vẫn là con mèo (scale invariance) Dữ liệu mà có cực nhiều với đầy đủ các patterns thì máy có thể tự học được các thay đổi đó nhưng thực tế thì không như vậy.</description></item><item><title>PRML - Chap 4: Linear Models for Classification</title><link>http://example.org/post/prml-chap-4-linear-models-for-classification/</link><pubDate>Sat, 29 Apr 2017 09:13:03 +0000</pubDate><guid>http://example.org/post/prml-chap-4-linear-models-for-classification/</guid><description>4.3.1 Fixed basis functions Bài toán là phân loại các input vector x Áp 1 hàm phi tuyến cố định $\phi(x)$ vào thì bài toán trở thành phân loại các $\phi(x)$ → đường decision boundary sẽ trờ thành tuyến tính → bài toán trở nên đơn giản đi rất nhiều 4.3.2 Logistic regression Thuật toán này rất quan trọng nên anh em cần chú ý hiểu kỹ Tên là regression nhưng lại là thuật toán classification Trở lại bài toán phân biệt 2 lớp: data set $\{\phi_ n, t_ n\}$ với $t_ n \in \{0,1\}$ và $n=1,\ldots,N$ Hàm likelihood trở thành $$ p(\mathbf{t}|w) = \prod_ {n=1}^N y_ n^{t_ n}{1-y_ n}^{1-t_ n} $$ với $\mathbf{t} = (t_ 1,\ldots,t_ N)^T$ và $y_ n = p(C_ 1 | \phi_ n)=y(\phi) = \sigma(w^T\phi)$ là xác suất để $\phi_ n$ rơi vào lớp $C_ 1$.</description></item><item><title>PRML - Chap 3: Linear Models for Regression</title><link>http://example.org/post/prml-chap3-linear-models-for-regression/</link><pubDate>Sat, 29 Apr 2017 08:58:59 +0000</pubDate><guid>http://example.org/post/prml-chap3-linear-models-for-regression/</guid><description>The Evidence Approximation Trong Bayesian đối với linear model, ta đã thấy sự xuất hiện của $\alpha$ và $\beta$ là các hyperparameters của prior và noise.
Trong chương này, ta sẽ cố gắng tìm các giá trị này dựa trên maximizing the magrinal likelihood function.
Framework này được gọi là evidence approximation.
Theo công thức Bayes:
$$ p(\alpha, \beta|\mathbf{t}) \propto p(\mathbf{t}|\alpha,\beta)p(\alpha,\beta) $$
$p(\mathbf{t}|\alpha,\beta)$ là marginal likelihood function
Evaluation of the evidence function Marginal likelihood function có thể triển khai theo $\mathbf{w}$ như sau:</description></item><item><title>PRML - Chap 2: Probability Distributions</title><link>http://example.org/post/prml-chap2-probability-distributions/</link><pubDate>Sat, 29 Apr 2017 08:41:59 +0000</pubDate><guid>http://example.org/post/prml-chap2-probability-distributions/</guid><description>2.3.3 Bayes&amp;rsquo; theorem for Gaussian variables Tóm lại Nếu $p(x)$ và $p(y|x)$ đều là các phân phối chuẩn thì $p(y)$ và $p(x|y)$ cũng là các phân phối chuẩn.
Công thức: Giả sử
$$ \begin{align} p(x) &amp;amp; = \mathcal{N}(x|\mathbf{\mu}, \mathbf{\Lambda} ^{-1}\big) \\ p(y|x)&amp;amp; = \mathcal{N}\big(y|\mathbf{A}x+\mathbf{b}, \mathbf{L} ^{-1}\big) \\ \end{align} $$
thì
$$ \begin{align} p(y) &amp;amp; = &amp;amp;\mathcal{N}\big(y|\mathbf{A}\mu+b,\mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}}\big)\\ p(x|y) &amp;amp; = &amp;amp;\mathcal{N}\big(x|\Sigma{ \mathbf{A}^{\mathrm{T}}\mathbf{L}(y-b)+\mathbf{\Lambda}\mu},\Sigma\big) \end{align} $$
với
$$ \Sigma = (\mathbf{\Lambda}+\mathbf{A}^{T}\mathbf{L}\mathbf{A})^{-1} $$
2.3.4 Maximum likelihood for the Gaussian Data set</description></item><item><title>PRML - Chap 1: Probability Theory</title><link>http://example.org/post/prml-chap1-probability-theory/</link><pubDate>Sat, 18 Feb 2017 15:56:40 +0000</pubDate><guid>http://example.org/post/prml-chap1-probability-theory/</guid><description>Ví dụ Có 2 hộp: Đỏ, Lam Có 2 loại quả: Táo(màu lá), Cam(màu cam)
Chọn 1 hộp bất kỳ rồi bốc 1 quả bất kỳ trong hộp đó
Biến ngẫu nhiên (Random variable) B: hộp, có thể nhận 1 trong 2 giá trị r(đỏ), b (lam) F: quả, có thể nhận 1 trong 2 giá trị a(táo), o (cam)
Ký hiệu xác suất Giả sử xác xuất chọn hộp đỏ trong 2 hộp là 4/10</description></item><item><title>Tôi đã tự học Deep Learning như thế nào</title><link>http://example.org/post/toi-da-tu-hoc-deep-learning-nhu-the-nao/</link><pubDate>Sun, 13 Nov 2016 13:53:00 +0000</pubDate><guid>http://example.org/post/toi-da-tu-hoc-deep-learning-nhu-the-nao/</guid><description>Nhân dịp TensorFlow vừa kỷ niệm 1 năm open source, mình viết bài này để kể lại 1 năm qua mình đã tự học Deep Learning như thế nào, vừa để tự tổng hợp kiến thức, vừa để cho các bạn nào quan tâm đến Machine Learning nói chung cũng như là Deep Learning nói riêng tham khảo.
Mình là 1 kỹ sư bên server-side, công việc chủ yếu dùng Rails, SQL và hoàn toàn không liên quan gì đến Deep Learning.</description></item></channel></rss>